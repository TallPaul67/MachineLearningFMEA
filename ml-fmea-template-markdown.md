# The ML FMEA Template

| Machine Learning Pipeline Step | Failure Mode Guide Words | Potential Failure Mode of the ML Model of Interest | Potential Effect on Higher Level System or Customer | Sev | Potential ML Pipeline Causes | Occ | Current ML Pipeline Best Practices & Process Controls | Det | ML RPN | Actions Recommended | Owner Target and Dates | Actions Taken | Sev | Occ | Det | Future ML RPN |
|-------------------------------|--------------------------|---------------------------------------------------|---------------------------------------------------|-----|----------------------------|-----|-----------------------------------------------------|-----|--------|---------------------|----------------------|---------------|-----|-----|-----|--------------|
| Collect Data Requests | Incorrect or missing | Insufficient data requests |  |  | Incomplete or insufficient data requested for collection can lead to an incomplete or insufficient training dataset. |  | Clear Problem Definition and Goal Alignment: The data collection requests must align with the goals of the machine learning project. The clearer the definition of the problem, the more specific the data request will be. This reduces the chance of irrelevant or noisy data, which can lead to model errors. Clear goal alignment ensures that data relevant to the learning task is collected, reducing the risk of introducing biases or irrelevant information into the model. |  |  |  |  |  |  |  |  |  |
| | Missing | Missing data collection requests |  |  | Incorrect prioritization of data collection requests can lead to biased or incorrect models. |  | Cross-Functional Input. Collaborate with domain experts to ensure that data requests reflect operational realities and domain-specific knowledge. This reduces the chance of missing important data dimensions or collecting incomplete datasets. By leveraging domain expertise, the data collected is more representative of real-world use cases, preventing models from making inaccurate assumptions due to incomplete or misunderstood data sources. |  |  |  |  |  |  |  |  |  |
| | Too late | Late data collection requests |  |  | Late or lengthy data collection requests can cause models to be trained on outdated information (such as changing of seasons). |  | Constraints for Collection: Data collection requests should include constraints to ensure the collected data intent is met. For example, data may have to be geographically constrained, seasonally constrained, or constrained by other conditions such as time of day or precipitation. |  |  |  |  |  |  |  |  |  |
| Collect Data | Incorrect or missing | Insufficient data collected |  |  | Incomplete or insufficient data collected can lead to an incomplete or insufficient training dataset. |  | Diverse and Representative Sampling: Ensure that data collection captures a wide variety of scenarios, especially edge cases and rare events. This ensures that the model learns from a comprehensive set of examples. By collecting data that covers the full spectrum of possible situations, models are less likely to fail when encountering novel or unexpected inputs. |  |  |  |  |  |  |  |  |  |
| | Incorrect | Insufficient data collected |  |  | Manual data collected is incorrect or does not match the intent of the collection request. |  | Automated Data Collection with Monitoring: Automate data collection wherever possible to minimize human error and introduce robust monitoring to detect anomalies or data drift during collection. Automation reduces the likelihood of introducing errors from manual data handling, while continuous monitoring ensures data quality and integrity remain high, preventing issues downstream in the pipeline. |  |  |  |  |  |  |  |  |  |
| | Too late | Late data collection |  |  | Late or lengthy collection of data can cause models to be trained on outdated or incorrect information (such as changing of seasons). |  | Constraints for Collection: Data collection should have clear constraints that may impact representative data collection. For example, data may have to be geographically constrained, seasonally constrained, or constrained by other conditions such as time of day or precipitation. |  |  |  |  |  |  |  |  |  |
| Ingest Data | Incorrect or missing | Insufficient data ingestion |  |  | Incomplete or inaccurate data collection can lead to biased or incorrect models. |  | Automate the Ingestion Process. Utilize robust ETL (Extract, Transform, Load) tools and frameworks to automate data collection. Automation ensures consistent and error-free data collection, preventing gaps and inconsistencies that could compromise model performance. Automation minimizes human error, ensuring that data is ingested accurately and efficiently, thus reducing the risk of introducing incomplete or erroneous data. |  |  |  |  |  |  |  |  |  |
| | Incorrect or missing | Insufficient data ingestion |  |  | Security breaches during data ingestion can compromise sensitive data, leading to ethical and legal issues. |  | Secure data transmission through encryption and ensure compliance with data protection regulations. Use access controls and audit logs to monitor data access. Protecting sensitive information during ingestion prevents unauthorized access and ensures the trustworthiness of the data used for training. |  |  |  |  |  |  |  |  |  |
| | Too late | Late data ingestion |  |  | Delays in data ingestion can cause models to be trained on outdated information. |  | Ensure Data Quality: Implement initial data checks for integrity, accuracy, and completeness. Tools like Apache Griffin or Great Expectations can automatically detect and rectify anomalies. Ensuring data quality from the start reduces the risk of the model learning incorrect patterns, improving the reliability of the model's predictions.<br>Examples:<br>• Sensor Synchronization Issues: Misalignment in the timing of data collected from multi-modal sensors, such as cameras, RADAR and LiDAR, can result in inconsistencies. To mitigate this, time-stamping sensor data and utilizing real-time synchronization methods help align the data more accurately. Additionally, leveraging multi-modal redundancy—cross-referencing data from different sensors—can identify and correct temporal misalignments.<br>• Dropped Messages: Data packet loss during transmission can create gaps in the data stream, potentially missing crucial information. Buffering and retry mechanisms are effective in ensuring that lost packets are re-transmitted. Additionally, multi-modal redundancy, using other sensors to verify missing data, can help bridge these gaps.<br>• Data Format Inconsistencies: Data from different sensors often comes in various formats, resolutions, or coordinate systems (autonomous vehicle example: point clouds from LiDAR, images from cameras, and radar readings). Furthermore during the development process the respective firmware get updated, leading to increased capabilities over the development cycle. These inconsistencies can create challenges during data fusion, leading to misinterpretations or loss of information. Implementing standardized data formats, using preprocessing pipelines to align resolutions, and running consistency checks can ensure uniformity across sensor data, enabling seamless integration in later stages. |  |  |  |  |  |  |  |  |  |
| Validate Data | Incorrect or missing | Insufficient data validation |  |  | Invalid or corrupt data can lead to erroneous model training and predictions. |  | Schema Validation: Enforce data structure and type constraints through schema definitions. Tools like JSON Schema or Apache Avro can automate schema validation. Schema validation catches and corrects errors early, preventing structural inconsistencies that could lead to model misinterpretation and incorrect learning. |  |  |  |  |  |  |  |  |  |
| | Incorrect or missing | Insufficient data validation |  |  | Undetected anomalies can introduce biases and reduce model performance. |  | Anomaly Detection: Implement automated checks to detect and handle anomalies such as outliers, missing values, and duplicate records. Identifying and mitigating anomalies ensures the model learns from clean, consistent data, reducing the risk of learning misleading patterns. |  |  |  |  |  |  |  |  |  |
| | Incorrect or missing | Data validation not provided |  |  | Lack of validation can result in using incompatible or irrelevant data for training. |  | Consistent Monitoring: Continuously monitor data quality metrics and set up alerts for significant deviations. Early detection and rectification of data issues help maintain the consistency and reliability of the data, reducing the risk of model degradation.<br>Examples:<br>1. Schema Validation: Enforcing data structure and type constraints through schema definitions ensures that the ingested data adheres to expected formats. Tools like JSON Schema or Apache Avro can automate schema validation, making it easier to detect and correct structural inconsistencies. Validating the schema in the datalake helps catch errors early, preventing issues like unexpected data structures that could lead to model misinterpretation and incorrect learning.<br>2. Anomaly Detection: Automated checks for anomalies such as outliers, missing values, and duplicate records can help ensure the integrity of the data. These can be achieved by applying a large scale offline virtual driver stack replaying the collected samples against the online stack and the driver in the loop. This allows to detect early misbehavior of the vehicle and important data samples for model refinement.<br>3. Consistency Issues: An autonomous driving stack involves multi-modal sensors and between real-world scene and individual subcomponent outputs can reveal for example in mapping depth information from LiDAR and stereo cameras, which can impact perception accuracy. During validation, cross-referencing multi-modal data ensure accurate alignment and data integrity.<br>4. Dirty Sensors: Physical contaminants like dust or moisture on sensor lenses can distort data. Validation systems that compare current sensor readings against pre-recorded baselines can identify deviations caused by dirty sensors. Automated cleaning systems can mitigate this issue by keeping sensor surfaces clear.<br>5. Sensor Configuration Errors: Misalignment or calibration errors between sensors can lead to measurement inaccuracies. Automated calibration tools and validation checks during data ingestion ensure consistent alignment. Cross-referencing depth information from multiple sensor types, like LiDAR and stereo cameras, further aids in validating sensor alignment. Mis-calibrated datasets are marked and further refined in pre processing steps.<br>6. Firmware and Software Issues: Software bugs or incompatibilities in sensor firmware can introduce data inconsistencies. Continuous integration testing and version control, paired with offline perception systems, help detect and address such anomalies during validation. |  |  |  |  |  |  |  |  |  |
| Preprocess Data | Missing | Insufficient data preprocessing |  |  | Poor handling of missing values can introduce biases. |  | Standardize Data Cleaning Procedures: Establish and follow standardized procedures for handling common data issues like missing values and outliers. Standardizing data cleaning procedures ensures consistency and reliability in the data used for training, reducing the risk of introducing biases and errors. As a healthcare industry approach example, active label cleaning is a proven approach to clean noisy annotation labels. |  |  |  |  |  |  |  |  |  |
| | Incorrect | Insufficient data preprocessing |  |  | Incorrect normalization or scaling can distort relationships in the data. |  | Automate Feature Engineering: Use automated feature engineering tools like Feature tools to systematically create and evaluate new features. Automation reduces the risk of overlooking critical data transformations, ensuring that the model captures all relevant information. |  |  |  |  |  |  |  |  |  |
| | Too little | Insufficient data preprocessing |  |  | Inadequate feature engineering can lead to suboptimal model performance. |  | Document Transformations: Keep detailed records of all transformations applied to the data. Documentation ensures reproducibility and facilitates debugging, helping identify and correct preprocessing steps that may introduce errors. As an example for autonomous vehicle environmental sensing via lidar, LidarAugment can be employed to augment 3D objects for robust detection. |  |  |  |  |  |  |  |  |  |
| Train Model | Incorrect | Insufficient model training |  |  | Overfitting or underfitting can occur if the model is not trained properly. |  | Use Cross-Validation: Employ techniques like k-fold cross-validation to ensure the model's performance is consistent across different data subsets. Cross-validation provides a more reliable estimate of the model's generalization ability, reducing the risk of overfitting. |  |  |  |  |  |  |  |  |  |
| | Incorrect | Insufficient model training |  |  | Incorrect algorithm selection can lead to poor model performance. |  | Hyperparameter Optimization: Automate hyperparameter tuning with tools like Grid Search, Random Search, or Bayesian Optimization. Proper tuning ensures the model performs optimally, preventing underfitting or overfitting. |  |  |  |  |  |  |  |  |  |
| | Too little | Insufficient model training |  |  | Poor parameter configuration can prevent the model from learning effectively. |  | Monitor Training Process: Track training metrics such as loss and accuracy in real-time. Tools like Tensor Board provide visual insights into the training process. Real-time monitoring allows for early detection of issues and timely intervention, ensuring the model trains correctly. |  |  |  |  |  |  |  |  |  |
| Tune Model | Too little | Insufficient model tuning |  |  | Suboptimal hyperparameter settings can degrade model performance. |  | Systematic Hyperparameter Tuning: Use systematic search methods or automated tools for hyperparameter tuning. Techniques like Bayesian Optimization or Hyperband systematically explore the hyperparameter space. Systematic tuning ensures optimal model performance and reduces the risk of suboptimal configurations that could degrade performance. |  |  |  |  |  |  |  |  |  |
| | Too much | Model is overtuned |  |  | Irrelevant or redundant features can increase model complexity and reduce accuracy. |  | Feature Selection: Evaluate the importance of features and remove irrelevant or redundant ones. Techniques like Recursive Feature Elimination (RFE) or LASSO can help. Feature selection reduces model complexity and improves generalization, preventing overfitting and enhancing accuracy. |  |  |  |  |  |  |  |  |  |
| | Too little | Insufficient model tuning |  |  | Lack of proper evaluation can result in a tuned model that does not generalize well. |  | Evaluate on Validation Set: Use a separate validation set to assess the performance of the tuned model. Proper evaluation prevents overfitting on the training data, ensuring the model's reliability in real-world applications. |  |  |  |  |  |  |  |  |  |
| Analyze Model | Too little | Insufficient model analysis |  |  | Over-reliance on a single metric can provide an incomplete picture of model performance. |  | Comprehensive Metrics: Use a range of evaluation metrics to cover different aspects of model performance. Multiple metrics provide a holistic understanding of model strengths and weaknesses, preventing optimization for a single aspect that might not capture all performance facets. |  |  |  |  |  |  |  |  |  |
| | Not provided | Missing model analysis |  |  | Failure to conduct thorough error analysis can leave critical issues unaddressed. |  | Error Analysis: Conduct a thorough analysis of the model's errors to identify limitations and areas for improvement. Understanding misclassification patterns helps implement targeted refinements, preventing repeated mistakes and improving overall model accuracy. |  |  |  |  |  |  |  |  |  |
| | Not provided | Missing model analysis |  |  | Lack of clear visualizations can make it difficult to interpret and act on model performance data. |  | Visualizations: Use visual tools like confusion matrices, ROC curves, and precision-recall curves to provide clear insights into the model's performance. Visualizations facilitate better interpretation and decision-making, helping identify and correct potential issues. |  |  |  |  |  |  |  |  |  |
| Validate Model | Not provided | Missing model validation |  |  | Overfitting to the training data can result in poor performance on new data. |  | Holdout Validation. Use a holdout validation set or cross-validation to ensure the model's performance generalizes well to new data. This practice provides a realistic estimate of how the model will perform in real-world scenarios, reducing the risk of overfitting. |  |  |  |  |  |  |  |  |  |
| | Too little | Insufficient model validation |  |  | Validation on an unrepresentative test set can provide a false sense of model reliability. |  | Real-World Testing. Validate the model with real-world data, if possible. Real-world testing highlights discrepancies between the training environment and real-world scenarios, preventing unexpected failures post-deployment. |  |  |  |  |  |  |  |  |  |
| | Too little | Insufficient model validation |  |  | Validation to a dependent or derivative test set can provide a false sense of model reliability. |  | Independent data set. Ensure an independent data set is collected, created, and curated for validation |  |  |  |  |  |  |  |  |  |
| | Too little | Insufficient model validation |  |  | Ignoring real-world scenarios can lead to unexpected failures post-deployment. |  | Real-World Testing. Validate the model with real-world data, if possible. Real-world testing highlights discrepancies between the training environment and real-world scenarios, preventing unexpected failures post-deployment. |  |  |  |  |  |  |  |  |  |
| Deploy Model | Too little | Insufficient model deployment |  |  | Inadequate infrastructure can lead to performance bottlenecks. |  | Continuous Integration / Continuous Deployment (CI/CD). Implement CI/CD pipelines to automate the deployment process. CI/CD ensures smooth updates and minimizes manual errors, preventing deployment failures and maintaining model consistency. |  |  |  |  |  |  |  |  |  |
| | Incorrect | Insufficient model deployment |  |  | Poor monitoring can result in undetected performance degradation. |  | Monitoring and Logging. Continuously monitor the deployed model's performance and log predictions. Set up alerts and analyze logs to detect issues early. Early detection allows for prompt intervention, preventing prolonged periods of poor performance and maintaining the model's reliability. |  |  |  |  |  |  |  |  |  |
| | Too little | Insufficient model deployment |  |  | Lack of rollback mechanisms can make it difficult to address issues post-deployment. |  | Implement Software Rollback. A software rollback feature can revert the software back to a known release with known limitations that may have adequate mitigations. |  |  |  |  |  |  |  |  |  |
| | Too little | Insufficient model deployment |  |  | Integration can mask model failures and introduce integration failures |  | Performance Thresholds. Define and adhere to performance thresholds that the model must meet before deployment. Ensuring the model meets required standards prevents the release of suboptimal models, maintaining high performance and reliability. |  |  |  |  |  |  |  |  |  |
| | Too little | Insufficient model deployment |  |  | Scaling of the deployed model can introduce unaccounted for effects. |  | Scalability and Reliability. Use scalable and reliable infrastructure to host the model. Cloud services like AWS, GCP, or Azure provide robust options. Scalable infrastructure ensures the model can handle varying loads and maintain performance, preventing downtime and degraded performance. |  |  |  |  |  |  |  |  |  |
| | Incorrect | Insufficient model deployment |  |  | Model learns incorrectly through on-line learning and failure is not identified through validation |  | Do not enable on-line learning. On-line learning can lead to unvalidated content in the algorithm which can cause unwanted output. By not enabling on-line learning additional deployment and testing cycles may be necessary but it will mitigate deploying untested algorithms that may have undesired outputs. |  |  |  |  |  |  |  |  |  |
| | Too little | Insufficient model deployment |  |  | Lack of feedback can result in undetected performance drift. |  | Feedback Loops. Establish feedback loops to collect data on the model's predictions and outcomes. Continuous learning and improvement ensure that the model adapts to changing conditions, maintaining its accuracy and relevance.<br>Monitor for Drift. Continuously monitor for data and concept drift to ensure early detection of performance degradation. Tools like Alibi Detect can identify changes in data dynamics. Monitoring for drift allows for timely retraining and updates to the model, preventing long-term degradation.<br>Performance indicators. Establish clear safety performance indicators that can capture ML issues. Safety performance indicators that have an established range can identify emerging issues with a model before they become a hazard. |  |  |  |  |  |  |  |  |  |
| | Incorrect | Insufficient model deployment |  |  | Ignoring user feedback can lead to models that do not meet user needs. |  | User Dashboard. Establish performance metrics from a user perspective. Establish an appropriate frequency to capture user feedback as well as the dashboard presentation view for key decision makers. |  |  |  |  |  |  |  |  |  |
| | Too late | Late model deployment |  |  | Delayed updates can cause the model to become obsolete. |  | Regular Updates. Schedule regular updates and retraining sessions for the model to incorporate new data and maintain performance. Regular updates ensure the model stays current with the latest data, preventing obsolescence and maintaining high performance. |  |  |  |  |  |  |  |  |  |
